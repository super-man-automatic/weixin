## 在main的基础上添加了大模型的流式回答
由于AI的实时响应问题以及用户的体验感，将非流式回答更改为流式回答是一个不错的选择。<br />
这个分支更改了大模型，因为本地ollama好像并不能直接使用websocket进行响应，也就做不到流式输出，所以使用星火AI代替了本地的ollama模型，使用websocket进行连接，要注意官方文档里面的鉴权信息，在代码中写入自己的APPID,APISECRET,APIKEY等信息，注意参数需要在官方文档中寻找。<br />
最后等待所有的流式回答接收完毕，整合到一起再通过后端服务器发送到数据库。
